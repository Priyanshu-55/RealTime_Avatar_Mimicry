# RealTime_Avatar_Mimicry

ðŸŽ­ RealTime Avatar Mimicry
ðŸ“œ Description
RealTime Avatar Mimicry is a live motion capture project that uses Mediapipe to extract landmarks and animates a 3D avatar to replicate human movements in real time. The project focuses on face and pose mimicry for creating immersive and interactive experiences.

âœ¨ Features
ðŸ“¸ Live Landmark Extraction:

Utilizes the Mediapipe API by Google to extract pose and facial landmarks directly from a live camera feed.
ðŸ”¢ Bone Angle Calculations:

For pose rotation, calculates angles between bones in 3D space using x, y, z axes.
Applies the tanh function to determine accurate bone rotations for realistic motion.
ðŸ•º Real-Time Avatar Mimicry:

Translates facial movements and pose data into smooth, real-time animations for a 3D avatar, making it mimic the userâ€™s movements dynamically.
ðŸš€ How It Works
Landmark Extraction:
Mediapipe is used to extract facial and pose landmarks in real time.

Angle Calculation:

For each bone, the rotation angles are calculated using vector math in 3D space.
Tanh functions are applied to ensure smooth and precise movements.
Avatar Animation:
The calculated rotations are applied to the 3D avatar bones, creating real-time mimicry.


ðŸŒŸ Future Enhancements
Add support for hand gestures and finer movements.
Optimize performance for low-latency mimicry.
Develop an intuitive UI for non-developers.
Expand compatibility with additional 3D formats and APIs.
ðŸ™Œ Contributions
Feel free to fork the repository, create issues, or make pull requests to contribute to the project. All contributions are welcome!

ðŸ“§ Contact
For questions or feedback, contact: priyanshuy2005@gmail.com
